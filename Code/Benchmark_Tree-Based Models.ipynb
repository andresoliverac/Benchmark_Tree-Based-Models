{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Individual Assingment\n",
    "# Models Benchmark\n",
    "\n",
    "### Andres Olivera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models \n",
    "\n",
    "# Ensemble\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, BaggingClassifier, AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "# Modeling\n",
    "from sklearn.model_selection import GridSearchCV    # Cross Validation and Tunning\n",
    "from sklearn.model_selection import train_test_split\n",
    "import random\n",
    "\n",
    "# Metrics\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix, roc_auc_score, accuracy_score, precision_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Graphs\n",
    "import matplotlib.pyplot as plt    #PLOT\n",
    "\n",
    "# Data prep\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import scale    #Standarize\n",
    "from sklearn.decomposition import PCA      #Feature Reduction\n",
    "from imblearn.over_sampling import SMOTE   #Oversampling\n",
    "from sklearn.feature_selection import RFE  # Feature selection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading Data\n",
    "test = pd.read_csv('C:/Users/aolivera/OneDrive - IESEG/IESEG/Courses/Second Semester/SL&ML/Kaggle/bank_mkt_test.csv')\n",
    "train = pd.read_csv('C:/Users/aolivera/OneDrive - IESEG/IESEG/Courses/Second Semester/SL&ML/Kaggle/bank_mkt_train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First Approach ===> Dummies, Oversampling, Scaling and PCA\n",
    "#### Second Approach ===> Frequency Enconding, Oversampling and RFE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Enginering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['age'] = pd.cut(train['age'],5, labels = ['young', 'adult','grown','mayor','grandpa'])\n",
    "test['age'] = pd.cut(test['age'],5, labels = ['young', 'adult','grown','mayor','grandpa'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get categorical variables in one DF\n",
    "categorical_df = train.select_dtypes(exclude=[np.number])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Spliting Train into Train and Validation\n",
    "X_train, X_test, y_train, y_test = train_test_split(train, train[\"subscribe\"], test_size=0.30, random_state=42)\n",
    "X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(train, train[\"subscribe\"], test_size=0.30, random_state=42)\n",
    "\n",
    "test_2 = test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Firt Approach "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.get_dummies(X_train, drop_first=True)\n",
    "X_test = pd.get_dummies(X_test, drop_first=True)\n",
    "\n",
    "test = pd.get_dummies(test, drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Second Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ---- Second Aprroach -------\n",
    "### FREQUENCY ENCODING\n",
    "\n",
    "### Econding tran set\n",
    "for i in categorical_df.columns:\n",
    "    # size of each category\n",
    "    encoding = X_train_2.groupby(i).size()\n",
    "    # get frequency of each category\n",
    "    encoding = encoding/len(X_train_2)\n",
    "    X_train_2[i] = X_train_2[i].map(encoding)\n",
    "\n",
    "\n",
    "### enconding the validation set\n",
    "for i in categorical_df.columns:\n",
    "    # size of each category\n",
    "    encoding = X_test_2.groupby(i).size()\n",
    "    # get frequency of each category\n",
    "    encoding = encoding/len(X_test_2)\n",
    "    X_test_2[i] = X_test_2[i].map(encoding)\n",
    "\n",
    "\n",
    "\n",
    "### enconding test holdout set\n",
    "for i in categorical_df.columns:\n",
    "    # size of each category\n",
    "    encoding = test_2.groupby(i).size()\n",
    "    # get frequency of each category\n",
    "    encoding = encoding/len(test_2)\n",
    "    test_2[i] = test_2[i].map(encoding)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### First Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ************  OverSampling  ****************\n",
    "\n",
    "#OverSampling with SMOTE\n",
    "method = SMOTE(sampling_strategy=0.4) # making 60-40 difference\n",
    "# Apply resampling to the training data only\n",
    "X_resampled, y_resampled = method.fit_sample(X_train, y_train)\n",
    "\n",
    "#Changing variable names back \n",
    "names = X_train.columns\n",
    "X_train = pd.DataFrame(X_resampled, columns = names)\n",
    "y_train = pd.DataFrame(y_resampled, columns = ['subscribe'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop target and id \n",
    "#Train and Validation\n",
    "X_train = X_train.drop({'subscribe', 'client_id'},axis =1)\n",
    "X_test = X_test.drop({'subscribe', 'client_id'},axis =1)\n",
    "\n",
    "#Test Holdout\n",
    "client_id = pd.DataFrame(test['client_id']) # Save client_ID to indentify the predictions at the end\n",
    "test = test.drop(['client_id'],axis=1) #Drop Client_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **********  Scaling  ***************  \n",
    "\n",
    "names = X_train.columns\n",
    "X_train =  pd.DataFrame(scale(X_train), columns = names)\n",
    "X_test =  pd.DataFrame(scale(X_test), columns = names)\n",
    "\n",
    "# #Houldout \n",
    "names_test = test.columns\n",
    "test =  pd.DataFrame(scale(test), columns = names_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#**********  FEATURE REDUCTION  ***********\n",
    "#PCA\n",
    "#From 54 variables to 3 \n",
    "\n",
    "#X_train\n",
    "pca = PCA(n_components=3)\n",
    "pca_data = pca.fit_transform(X_train)\n",
    "X_train = pd.DataFrame(data = pca_data, columns = ['var1', 'var2','var3'])\n",
    "\n",
    "#Validation\n",
    "pca_data_val = pca.fit_transform(X_test)\n",
    "X_test = pd.DataFrame(data = pca_data_val, columns = ['var1', 'var2','var3'])\n",
    "\n",
    "#Test holdout\n",
    "pca_data_test = pca.fit_transform(test)\n",
    "test = pd.DataFrame(data = pca_data_test, columns = ['var1', 'var2','var3'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Second Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ************  OverSampling  ****************\n",
    "\n",
    "# Apply resampling to the training data only\n",
    "X_resampled_2, y_resampled_2 = method.fit_sample(X_train_2, y_train_2)\n",
    "\n",
    "#Changing variable names back \n",
    "names = X_train_2.columns\n",
    "X_train_2 = pd.DataFrame(X_resampled_2, columns = names)\n",
    "y_train_2 = pd.DataFrame(y_resampled_2, columns = ['subscribe'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop target and id \n",
    "#Train and Validation\n",
    "X_train_2 = X_train_2.drop({'subscribe', 'client_id'},axis =1)\n",
    "X_test_2 = X_test_2.drop({'subscribe', 'client_id'},axis =1)\n",
    "\n",
    "#Test Holdout\n",
    "client_id = pd.DataFrame(test_2['client_id']) # Save client_ID to indentify the predictions at the end\n",
    "test_2 = test_2.drop(['client_id'],axis=1) #Drop Client_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#**********  FEATURE REDUCTION  ***********\n",
    "\n",
    "#This method uses the feature importance of the model (estimator). In this case is the feature importances that RF gives to all the variables. If it was Logistic Regression it would be the coefficient. \n",
    "rfc=RandomForestClassifier()\n",
    "# Assign Feature Selection method\n",
    "rfe_selector = RFE(estimator= rfc, n_features_to_select=15, step=10, verbose=5)\n",
    "# Fit it and get best X freatures\n",
    "rfe_selector.fit(X_train_2, y_train_2)\n",
    "\n",
    "rfe_support = rfe_selector.get_support()\n",
    "rfe_feature = X_train_2.loc[:,rfe_support].columns.tolist()\n",
    "print(str(len(rfe_feature)), 'selected features')\n",
    "\n",
    "X_train_2 = X_train_2[rfe_feature] \n",
    "\n",
    "# Get same columns in validation test\n",
    "X_test_2 = X_test_2[rfe_feature]\n",
    "\n",
    "#Get same columns in test holdout \n",
    "test_2 = test_2[rfe_feature]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#************  Models & Parameters   *************\n",
    "\n",
    "# --- Tree based models ----\n",
    "\n",
    "# Decision Tree\n",
    "tree = DecisionTreeClassifier()\n",
    "treeParam = {'criterion' : ['gini', 'entropy'], 'max_depth' : [4, 6, 8]}\n",
    "#Ensemble\n",
    "\n",
    "# Baggin\n",
    "bag = BaggingClassifier()\n",
    "bagParam = {'n_estimators': [100, 200, 500, 1000], 'max_depth' : [4, 6, 8]}\n",
    "\n",
    "#Random Forest\n",
    "rfc=RandomForestClassifier()\n",
    "rfcParam = {'n_estimators': [100, 200, 500, 1000],'max_features': ['log2', 'sqrt'],'max_depth' : [4, 6, 8]}\n",
    "\n",
    "\n",
    "# Boosting\n",
    "#Adaboost\n",
    "adab = AdaBoostClassifier()\n",
    "adabParam = {'n_estimators': [100, 200, 500, 1000], 'learning_rate': [0.1, 0.05, 0.01, 0.001],'max_depth' : [5,15,25]}\n",
    "\n",
    "#Gradient Boosting\n",
    "gb = GradientBoostingClassifier()\n",
    "gbParam = {'n_estimators': [100, 200, 500, 1000], 'max_features': ['log2', 'sqrt'],'learning_rate': [0.1, 0.05, 0.01, 0.001], 'max_depth': [4, 6, 8]}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#KNN\n",
    "knn = KNeighborsClassifier()\n",
    "knnParams = {'n_neighbors':[4,5,7],'algorithm':['auto', 'kd_tree']}\n",
    "\n",
    "#Logistic Regression\n",
    "lr = LogisticRegression()\n",
    "c_space = np.logspace(-5, 8, 15, 20)\n",
    "lrParams = {'C': c_space, 'penalty' : ['l1', 'l2']}\n",
    "\n",
    "\n",
    "#Support Vector Machine\n",
    "svm = SVC()\n",
    "svmParams = {'C': [0.1, 10], 'gamma': [1,0.1],'kernel': ['poly', 'sigmoid']}\n",
    "\n",
    "# MLP Classifier\n",
    "mlp = MLPClassifier()\n",
    "mlpParams = {'max_iter': [10],  'hidden_layer_sizes':np.arange(10, 15), 'random_state':[0,1]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tree, Bagging and RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#************  Models & Parameters   *************\n",
    "\n",
    "# --- Tree based models ----\n",
    "\n",
    "# Decision Tree\n",
    "tree = DecisionTreeClassifier()\n",
    "treeParam = {'criterion' : ['gini', 'entropy'], 'max_depth' : [4, 6, 8]}\n",
    "#Ensemble\n",
    "\n",
    "# Baggin\n",
    "bag = BaggingClassifier()\n",
    "bagParam = {'n_estimators': [100, 200, 500, 1000]}\n",
    "\n",
    "#Random Forest\n",
    "rfc=RandomForestClassifier()\n",
    "rfcParam = {'n_estimators': [100, 200, 500, 1000],'max_features': ['log2', 'sqrt'],'max_depth' : [4, 6, 8]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ********************* Hyperparameter Tunning ******************\n",
    "\n",
    "random.seed(30)\n",
    "#list of models for loop\n",
    "models = (tree, bag, rfc)\n",
    "models2 = ('Tree', 'Baggin','Random Forest')\n",
    "#list of param for loop\n",
    "param = (treeParam, bagParam, rfcParam)\n",
    "\n",
    "#List to append results\n",
    "auc = []\n",
    "accuracy = []\n",
    "modelparam= []\n",
    "recall = []\n",
    "precision = []\n",
    "f1 = []\n",
    "\n",
    "for i in range(len(models)):\n",
    "    #Loop list of models and its parameters\n",
    "    cv = GridSearchCV(models[i], param[i], cv=4)\n",
    "    #Train model with train set\n",
    "    cvModel = cv.fit(X_train, y_train)\n",
    "    \n",
    "    #Train:\n",
    "    #Get probabilitie predictions on train\n",
    "    preds_train = cvModel.predict_proba(X_train)[:,1]\n",
    "    #Get numeric prediction on tran\n",
    "    predi_num_t = cvModel.predict(X_train)\n",
    "    \n",
    "    #Test\n",
    "    #Get probability predictions on test\n",
    "    preds = cvModel.predict_proba(X_test)[:,1]\n",
    "    #Get numeric prediction on test\n",
    "    predi_num = cvModel.predict(X_test)\n",
    "    \n",
    "    \n",
    "    #Save best model\n",
    "    #save predictions of number of train\n",
    "    #save probability predictions of Train\n",
    "    #save predictions of number of test\n",
    "    #save probability predictions of Test\n",
    "    if i == 0: \n",
    "        model1 = cvModel\n",
    "        pred1_num_t = predi_num_t\n",
    "        pred1_t = preds_train\n",
    "        pred1_num = predi_num\n",
    "        pred1 = preds\n",
    "    elif i == 1:\n",
    "        model2 = cvModel\n",
    "        pred2_num_t = predi_num_t\n",
    "        pred2_t = preds_train\n",
    "        pred2_num = predi_num\n",
    "        pred2 = preds\n",
    "    elif i == 2: \n",
    "        model3 = cvModel\n",
    "        pred3_num_t = predi_num_t\n",
    "        pred3_t = preds_train\n",
    "        pred3_num = predi_num\n",
    "        pred3 = preds\n",
    "    elif i == 3: \n",
    "        model4 = cvModel\n",
    "        pred4_num_t = predi_num_t\n",
    "        pred4_t = preds_train\n",
    "        pred4_num = predi_num \n",
    "        pred4 = preds\n",
    "    else:\n",
    "        model5 = cvModel\n",
    "        pred5_num_t = predi_num_t\n",
    "        pred5_t = preds_train\n",
    "        pred5_num = predi_num\n",
    "        pred5 = preds\n",
    "\n",
    "    #METRICS\n",
    "    #AUC\n",
    "    test_roc_auc = roc_auc_score(y_test, preds)\n",
    "    auc.append([roc_auc_score(y_test, preds)])\n",
    "    #Accuracy\n",
    "    accuracy.append([cvModel.score(X_test, y_test)])\n",
    "    #Parameters best model\n",
    "    modelparam.append(cvModel.best_estimator_)\n",
    "    #Classification Report (precision    recall  f1-score   support)\n",
    "    recall.append([recall_score(y_test,predi_num)])\n",
    "    precision.append([precision_score(y_test,predi_num)])\n",
    "    f1.append([f1_score(y_test,predi_num)])\n",
    "    print(models2[i])\n",
    "        \n",
    "#Creating Dataframe Metrics\n",
    "metrics = pd.DataFrame(list(zip(models2,auc, accuracy, recall, precision,f1, modelparam )), columns =['Models', 'AUC','Accuracy','Recall','Precision','F1','Param']) \n",
    "\n",
    "#Round and Numeric variables\n",
    "for x in metrics.iloc[:,1:6].columns:\n",
    "     metrics[x] = metrics[x].astype(str)\n",
    "     metrics[x] = metrics[x].str.replace(\"]\",\" \")\n",
    "     metrics[x] = metrics[x].str.replace(\"[\",\" \")\n",
    "     metrics[x] = metrics[x].astype(float)\n",
    "     metrics[x] = round(metrics[x],3)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Boosting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boosting\n",
    "#Adaboost\n",
    "adab = AdaBoostClassifier()\n",
    "adabParam = {'n_estimators': [100, 500, 1000], 'learning_rate': [0.1, 0.01, 0.001]}\n",
    "\n",
    "#Gradient Boosting\n",
    "gbm = GradientBoostingClassifier()\n",
    "gbmParam = {'n_estimators': [100, 500, 1000], 'learning_rate': [0.1, 0.05, 0.01, 0.001], 'max_depth': [4, 6, 8]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ********************* Hyperparameter Tunning ******************\n",
    "\n",
    "random.seed(30)\n",
    "#list of models for loop\n",
    "models = (adab, gbm)\n",
    "models2 = ('AdaBoost', 'GBM')\n",
    "#list of param for loop\n",
    "param = (adabParam, gbmParam)\n",
    "\n",
    "#List to append results\n",
    "auc = []\n",
    "accuracy = []\n",
    "modelparam= []\n",
    "recall = []\n",
    "precision = []\n",
    "f1 = []\n",
    "\n",
    "for i in range(len(models)):\n",
    "    #Loop list of models and its parameters\n",
    "    cv = GridSearchCV(models[i], param[i], cv=4)\n",
    "    #Train model with train set\n",
    "    cvModel = cv.fit(X_train, y_train)\n",
    "    \n",
    "    #Train:\n",
    "    #Get probabilitie predictions on train\n",
    "    preds_train = cvModel.predict_proba(X_train)[:,1]\n",
    "    #Get numeric prediction on tran\n",
    "    predi_num_t = cvModel.predict(X_train)\n",
    "    \n",
    "    #Test\n",
    "    #Get probability predictions on test\n",
    "    preds = cvModel.predict_proba(X_test)[:,1]\n",
    "    #Get numeric prediction on test\n",
    "    predi_num = cvModel.predict(X_test)\n",
    "    \n",
    "    \n",
    "    #Save best model\n",
    "    #save predictions of number of train\n",
    "    #save probability predictions of Train\n",
    "    #save predictions of number of test\n",
    "    #save probability predictions of Test\n",
    "    if i == 0: \n",
    "        model1 = cvModel\n",
    "        pred1_num_t = predi_num_t\n",
    "        pred1_t = preds_train\n",
    "        pred1_num = predi_num\n",
    "        pred1 = preds\n",
    "    elif i == 1:\n",
    "        model2 = cvModel\n",
    "        pred2_num_t = predi_num_t\n",
    "        pred2_t = preds_train\n",
    "        pred2_num = predi_num\n",
    "        pred2 = preds\n",
    "    elif i == 2: \n",
    "        model3 = cvModel\n",
    "        pred3_num_t = predi_num_t\n",
    "        pred3_t = preds_train\n",
    "        pred3_num = predi_num\n",
    "        pred3 = preds\n",
    "    elif i == 3: \n",
    "        model4 = cvModel\n",
    "        pred4_num_t = predi_num_t\n",
    "        pred4_t = preds_train\n",
    "        pred4_num = predi_num \n",
    "        pred4 = preds\n",
    "    else:\n",
    "        model5 = cvModel\n",
    "        pred5_num_t = predi_num_t\n",
    "        pred5_t = preds_train\n",
    "        pred5_num = predi_num\n",
    "        pred5 = preds\n",
    "\n",
    "    #METRICS\n",
    "    #AUC\n",
    "    test_roc_auc = roc_auc_score(y_test, preds)\n",
    "    auc.append([roc_auc_score(y_test, preds)])\n",
    "    #Accuracy\n",
    "    accuracy.append([cvModel.score(X_test, y_test)])\n",
    "    #Parameters best model\n",
    "    modelparam.append(cvModel.best_estimator_)\n",
    "    #Classification Report (precision    recall  f1-score   support)\n",
    "    recall.append([recall_score(y_test,predi_num)])\n",
    "    precision.append([precision_score(y_test,predi_num)])\n",
    "    f1.append([f1_score(y_test,predi_num)])\n",
    "    print(models2[i])\n",
    "        \n",
    "#Creating Dataframe Metrics\n",
    "metrics = pd.DataFrame(list(zip(models2,auc, accuracy, recall, precision,f1, modelparam )), columns =['Models', 'AUC','Accuracy','Recall','Precision','F1','Param']) \n",
    "\n",
    "#Round and Numeric variables\n",
    "for x in metrics.iloc[:,1:6].columns:\n",
    "     metrics[x] = metrics[x].astype(str)\n",
    "     metrics[x] = metrics[x].str.replace(\"]\",\" \")\n",
    "     metrics[x] = metrics[x].str.replace(\"[\",\" \")\n",
    "     metrics[x] = metrics[x].astype(float)\n",
    "     metrics[x] = round(metrics[x],3)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of Results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##PLOT of TRAIN and TEST ROC RF (second best performer)\n",
    "\n",
    "# Function\n",
    "def plot_train_test(y_test, pred_test, y_train, pred_train ):\n",
    "    fpr, tpr, thresholds = roc_curve(y_test,pred_test)\n",
    "    fpr2, tpr2, thresholds = roc_curve(y_train, pred_train)\n",
    "\n",
    "    # Plot ROC curve\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.plot(fpr, tpr, label= \"Test\")\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.plot(fpr2, tpr2, label = \"Train\", color='orange')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve RF')\n",
    "    plt.legend()\n",
    "    return plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AdaBoost\n",
    "plot_train_test(y_test, pred1, y_train, pred1_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gradient Boosting\n",
    "plot_train_test(y_test, pred2, y_train, pred2_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Random Forest\n",
    "plot_train_test(y_test, pred3, y_train, pred3_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#PLOT of BENCHMARK TEST ROC CURVS\n",
    "\n",
    "# Generate ROC curve values: fpr, tpr, thresholds\n",
    "fpr, tpr, thresholds = roc_curve(y_test,pred1)\n",
    "b, bb, thresholds = roc_curve(y_test,pred2)\n",
    "c, cc, thresholds = roc_curve(y_test,pred3)\n",
    "#d, dd, thresholds = roc_curve(y_test,pred4)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr, tpr, label= \"AdaBoost\")\n",
    "plt.plot(b, bb, label = \"GBM\", color='orange')\n",
    "plt.plot(c, cc, label = \"RF\", color='red')\n",
    "#plt.plot(d, dd, label = \"KNN\", color='green')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve Test Sets')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#************  Models & Parameters   *************\n",
    "\n",
    "# --- Tree based models ----\n",
    "\n",
    "# Decision Tree\n",
    "tree = DecisionTreeClassifier()\n",
    "treeParam = {'criterion' : ['gini', 'entropy'], 'max_depth' : [4, 6, 8]}\n",
    "#Ensemble\n",
    "\n",
    "# Baggin\n",
    "bag = BaggingClassifier()\n",
    "bagParam = {'n_estimators': [100, 200, 500, 1000]}\n",
    "\n",
    "#Random Forest\n",
    "rfc=RandomForestClassifier()\n",
    "rfcParam = {'n_estimators': [100, 200, 500, 1000],'max_features': ['log2', 'sqrt'],'max_depth' : [4, 6, 8]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ********************* Hyperparameter Tunning ******************\n",
    "\n",
    "random.seed(30)\n",
    "#list of models for loop\n",
    "models = (tree, bag, rfc)\n",
    "models2 = ('Tree', 'Baggin','Random Forest')\n",
    "#list of param for loop\n",
    "param = (treeParam, bagParam, rfcParam)\n",
    "\n",
    "#List to append results\n",
    "auc = []\n",
    "accuracy = []\n",
    "modelparam= []\n",
    "recall = []\n",
    "precision = []\n",
    "f1 = []\n",
    "\n",
    "for i in range(len(models)):\n",
    "    #Loop list of models and its parameters\n",
    "    cv = GridSearchCV(models[i], param[i], cv=4)\n",
    "    #Train model with train set\n",
    "    cvModel = cv.fit(X_train_2, y_train_2)\n",
    "    \n",
    "    #Train:\n",
    "    #Get probabilitie predictions on train\n",
    "    preds_train = cvModel.predict_proba(X_train_2)[:,1]\n",
    "    #Get numeric prediction on tran\n",
    "    predi_num_t = cvModel.predict(X_train_2)\n",
    "    \n",
    "    #Test\n",
    "    #Get probability predictions on test\n",
    "    preds = cvModel.predict_proba(X_test_2)[:,1]\n",
    "    #Get numeric prediction on test\n",
    "    predi_num = cvModel.predict(X_test_2)\n",
    "    \n",
    "    \n",
    "    #Save best model\n",
    "    #save predictions of number of train\n",
    "    #save probability predictions of Train\n",
    "    #save predictions of number of test\n",
    "    #save probability predictions of Test\n",
    "    if i == 0: \n",
    "        model1 = cvModel\n",
    "        pred1_num_t = predi_num_t\n",
    "        pred1_t = preds_train\n",
    "        pred1_num = predi_num\n",
    "        pred1 = preds\n",
    "    elif i == 1:\n",
    "        model2 = cvModel\n",
    "        pred2_num_t = predi_num_t\n",
    "        pred2_t = preds_train\n",
    "        pred2_num = predi_num\n",
    "        pred2 = preds\n",
    "    elif i == 2: \n",
    "        model3 = cvModel\n",
    "        pred3_num_t = predi_num_t\n",
    "        pred3_t = preds_train\n",
    "        pred3_num = predi_num\n",
    "        pred3 = preds\n",
    "    elif i == 3: \n",
    "        model4 = cvModel\n",
    "        pred4_num_t = predi_num_t\n",
    "        pred4_t = preds_train\n",
    "        pred4_num = predi_num \n",
    "        pred4 = preds\n",
    "    else:\n",
    "        model5 = cvModel\n",
    "        pred5_num_t = predi_num_t\n",
    "        pred5_t = preds_train\n",
    "        pred5_num = predi_num\n",
    "        pred5 = preds\n",
    "\n",
    "    #METRICS\n",
    "    #AUC\n",
    "    test_roc_auc = roc_auc_score(y_test_2, preds)\n",
    "    auc.append([roc_auc_score(y_test_2, preds)])\n",
    "    #Accuracy\n",
    "    accuracy.append([cvModel.score(X_test_2, y_test)])\n",
    "    #Parameters best model\n",
    "    modelparam.append(cvModel.best_estimator_)\n",
    "    #Classification Report (precision    recall  f1-score   support)\n",
    "    recall.append([recall_score(y_test_2,predi_num)])\n",
    "    precision.append([precision_score(y_test_2,predi_num)])\n",
    "    f1.append([f1_score(y_test_2,predi_num)])\n",
    "    print(models2[i])\n",
    "        \n",
    "#Creating Dataframe Metrics\n",
    "metrics = pd.DataFrame(list(zip(models2,auc, accuracy, recall, precision,f1, modelparam )), columns =['Models', 'AUC','Accuracy','Recall','Precision','F1','Param']) \n",
    "\n",
    "#Round and Numeric variables\n",
    "for x in metrics.iloc[:,1:6].columns:\n",
    "     metrics[x] = metrics[x].astype(str)\n",
    "     metrics[x] = metrics[x].str.replace(\"]\",\" \")\n",
    "     metrics[x] = metrics[x].str.replace(\"[\",\" \")\n",
    "     metrics[x] = metrics[x].astype(float)\n",
    "     metrics[x] = round(metrics[x],3)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.to_csv('C:/Users/aolivera/OneDrive - IESEG/IESEG/Courses/Second Semester/SL&ML/Individual Assignment/RANDOM.csv', index = False)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boosting\n",
    "#Adaboost\n",
    "adab = AdaBoostClassifier()\n",
    "adabParam = {'n_estimators': [100, 500, 1000], 'learning_rate': [0.1,  0.01, 0.001]}\n",
    "\n",
    "#Gradient Boosting\n",
    "gbm = GradientBoostingClassifier()\n",
    "gbmParam = {'n_estimators': [100, 200, 500], 'max_features': ['log2', 'sqrt'],'learning_rate': [0.1, 0.01, 0.001], 'max_depth': [4, 6, 8]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ********************* Hyperparameter Tunning ******************\n",
    "\n",
    "random.seed(30)\n",
    "#list of models for loop\n",
    "models = (adab, gbm)\n",
    "models2 = ('AdaBoost', 'GBM')\n",
    "#list of param for loop\n",
    "param = (adabParam, gbmParam)\n",
    "\n",
    "#List to append results\n",
    "auc = []\n",
    "accuracy = []\n",
    "modelparam= []\n",
    "recall = []\n",
    "precision = []\n",
    "f1 = []\n",
    "\n",
    "for i in range(len(models)):\n",
    "    #Loop list of models and its parameters\n",
    "    cv = GridSearchCV(models[i], param[i], cv=4)\n",
    "    #Train model with train set\n",
    "    cvModel = cv.fit(X_train_2, y_train_2)\n",
    "    \n",
    "    #Train:\n",
    "    #Get probabilitie predictions on train\n",
    "    preds_train = cvModel.predict_proba(X_train_2)[:,1]\n",
    "    #Get numeric prediction on tran\n",
    "    predi_num_t = cvModel.predict(X_train_2)\n",
    "    \n",
    "    #Test\n",
    "    #Get probability predictions on test\n",
    "    preds = cvModel.predict_proba(X_test_2)[:,1]\n",
    "    #Get numeric prediction on test\n",
    "    predi_num = cvModel.predict(X_test_2)\n",
    "    \n",
    "    \n",
    "    #Save best model\n",
    "    #save predictions of number of train\n",
    "    #save probability predictions of Train\n",
    "    #save predictions of number of test\n",
    "    #save probability predictions of Test\n",
    "    if i == 0: \n",
    "        model1 = cvModel\n",
    "        pred1_num_t = predi_num_t\n",
    "        pred1_t = preds_train\n",
    "        pred1_num = predi_num\n",
    "        pred1 = preds\n",
    "    elif i == 1:\n",
    "        model2 = cvModel\n",
    "        pred2_num_t = predi_num_t\n",
    "        pred2_t = preds_train\n",
    "        pred2_num = predi_num\n",
    "        pred2 = preds\n",
    "    elif i == 2: \n",
    "        model3 = cvModel\n",
    "        pred3_num_t = predi_num_t\n",
    "        pred3_t = preds_train\n",
    "        pred3_num = predi_num\n",
    "        pred3 = preds\n",
    "    elif i == 3: \n",
    "        model4 = cvModel\n",
    "        pred4_num_t = predi_num_t\n",
    "        pred4_t = preds_train\n",
    "        pred4_num = predi_num \n",
    "        pred4 = preds\n",
    "    else:\n",
    "        model5 = cvModel\n",
    "        pred5_num_t = predi_num_t\n",
    "        pred5_t = preds_train\n",
    "        pred5_num = predi_num\n",
    "        pred5 = preds\n",
    "\n",
    "    #METRICS\n",
    "    #AUC\n",
    "    test_roc_auc = roc_auc_score(y_test_2, preds)\n",
    "    auc.append([roc_auc_score(y_test_2, preds)])\n",
    "    #Accuracy\n",
    "    accuracy.append([cvModel.score(X_test_2, y_test)])\n",
    "    #Parameters best model\n",
    "    modelparam.append(cvModel.best_estimator_)\n",
    "    #Classification Report (precision    recall  f1-score   support)\n",
    "    recall.append([recall_score(y_test_2,predi_num)])\n",
    "    precision.append([precision_score(y_test_2,predi_num)])\n",
    "    f1.append([f1_score(y_test_2,predi_num)])\n",
    "    print(models2[i])\n",
    "        \n",
    "#Creating Dataframe Metrics\n",
    "metrics = pd.DataFrame(list(zip(models2,auc, accuracy, recall, precision,f1, modelparam )), columns =['Models', 'AUC','Accuracy','Recall','Precision','F1','Param']) \n",
    "\n",
    "#Round and Numeric variables\n",
    "for x in metrics.iloc[:,1:6].columns:\n",
    "     metrics[x] = metrics[x].astype(str)\n",
    "     metrics[x] = metrics[x].str.replace(\"]\",\" \")\n",
    "     metrics[x] = metrics[x].str.replace(\"[\",\" \")\n",
    "     metrics[x] = metrics[x].astype(float)\n",
    "     metrics[x] = round(metrics[x],3)\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.to_csv('C:/Users/aolivera/OneDrive - IESEG/IESEG/Courses/Second Semester/SL&ML/Individual Assignment/ada_gbm.csv', index = False)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of Results 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "##PLOT of TRAIN and TEST ROC RF (second best performer)\n",
    "\n",
    "# # Generate ROC curve values: fpr, tpr, thresholds\n",
    "def plot_train_test(y_test, pred_test, y_train, pred_train ):\n",
    "    fpr, tpr, thresholds = roc_curve(y_test,pred_test)\n",
    "    fpr2, tpr2, thresholds = roc_curve(y_train, pred_train)\n",
    "\n",
    "    # Plot ROC curve\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.plot(fpr, tpr, label= \"Test\")\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.plot(fpr2, tpr2, label = \"Train\", color='orange')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve RF')\n",
    "    plt.legend()\n",
    "    return plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#AdaBoost\n",
    "plot_train_test(y_test, pred1, y_train, pred1_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gradient Boosting\n",
    "plot_train_test(y_test, pred2, y_train, pred2_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#PLOT of BENCHMARK TEST ROC CURVS\n",
    "\n",
    "# Generate ROC curve values: fpr, tpr, thresholds\n",
    "fpr, tpr, thresholds = roc_curve(y_test,pred1)\n",
    "b, bb, thresholds = roc_curve(y_test,pred2)\n",
    "c, cc, thresholds = roc_curve(y_test,pred3)\n",
    "#d, dd, thresholds = roc_curve(y_test,pred4)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.plot(fpr, tpr, label= \"AdaBoost\")\n",
    "plt.plot(b, bb, label = \"GBM\", color='orange')\n",
    "plt.plot(c, cc, label = \"RF\", color='red')\n",
    "#plt.plot(d, dd, label = \"KNN\", color='green')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve Test Sets')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions to Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *************  Method 1 ***************\n",
    "# Predicting with best models\n",
    "final_models = [model1, model2, model3, model4, model5]\n",
    "predictions_df = ['m', 'n', 'o', \"p\", 'j']\n",
    "for p in range(0,5):\n",
    "    predictions_kaggle = final_models[p].predict_proba(test)[:,1]\n",
    "    predictions_df[p] = pd.DataFrame(predictions_kaggle, columns = ['subscribe'])\n",
    "    predictions_df[p]['client_id'] = client_id  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Downloading results\n",
    "predictions_df[1].to_csv('C:/Users/aolivera/OneDrive - IESEG/IESEG/Courses/Second Semester/SL&ML/Kaggle/model_pca_smot10.csv', index = False)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# *************    Method 2   **********************\n",
    "\n",
    "# All process again on full Train data\n",
    "train = pd.read_csv('C:/Users/aolivera/OneDrive - IESEG/IESEG/Courses/Second Semester/SL&ML/Kaggle/bank_mkt_train.csv')\n",
    "train = pd.get_dummies(train, drop_first=True)\n",
    "y_train = train['subscribe']\n",
    "#Drop target and id \n",
    "train = train.drop({'subscribe', 'client_id'},axis =1)\n",
    "names = train.columns\n",
    "#scale\n",
    "train =  pd.DataFrame(scale(train), columns = names)\n",
    "#pca\n",
    "pca = PCA(n_components=2)\n",
    "pca_data = pca.fit_transform(train)\n",
    "train = pd.DataFrame(data = pca_data, columns = ['princ_comp_1', 'princ_comp_2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fitting the best model with the selected hyperparameters with the original full Train Data and making predictions\n",
    "final_models = [model1, model2, model3, model4, model5]\n",
    "predictions_df = ['m', 'n', 'o', \"p\", 'j']\n",
    "for p in range(0,5):\n",
    "    final_models[p] = final_models[p].fit(train,y_train)\n",
    "    predictions_kaggle = final_models[p].predict_proba(test)[:,1]\n",
    "    predictions_df[p] = pd.DataFrame(predictions_kaggle, columns = ['subscribe'])\n",
    "    predictions_df[p]['client_id'] = client_id  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Downloading results\n",
    "predictions_df[1].to_csv('C:/Users/aolivera/OneDrive - IESEG/IESEG/Courses/Second Semester/SL&ML/Kaggle/model_pca_smot10.csv', index = False)   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
